\section{Methods}

In this section the methods used throughout the heuristic usability evaluation are specified and explained.


\subsection{Experts}


We recruited a team of 6 people. All of them are students of computer science who are doing the same project as us. Thus, the evaluators fall in the group of the “novice” evaluators, henceforth approximately 55 of all the issues\cite{nielsen94} are expected to be identified.

\subsection{Procedure}


The team that evaluated us is supposed to give us heuristic usability evaluation on our project prototype according to the rules we sent them.

We provided them with a prototype of our application in a form of wireframes with detailed explanations of functionality on every scene and each component on it. In those we included all the functionality we already implemented in our application as well as the aspects we are planning to implement. The general design of all components is also present in those wireframes, so they can also evaluate our design.

Every individual is supposed to find as many problems by using any of the following heuristics\cite{ten-heuristics}: 

\begin{itemize}
\item Visibility of system status
\item Match between system and the real world
\item User control and freedom
\item Consistency and standards
\item Error prevention
\item Recognition rather than recall
\item Flexibility and efficiency of use
\item Aesthetic and minimalist design
\item Help users recognize, diagnose, and recover from errors
\item Help and documentation
\end{itemize}

It is important to note that each expert needs to individually look at materials we have provided and evaluate the product in isolation from other evaluators, since the reviews of different people were shown to be statistically nearly independent, thus leading to significantly higher proportion of issues found in comparison to the case with single evaluator\cite{nielsen92}. 

For each problem found, the evaluator should write a short title on what the problem is along with the detailed description of the problem, description of the difficulties created by the problem, specific context in which the problem occurs and suggest possible causes for the occurrence of the problem. Moreover, the estimate of the severity of the problem should be provided, being either a major or minor usability problem.

	The aim of the evaluation is to measure and quantify the usability of the user interface of the prototype. The evaluators are instructed to report their findings as a list of usability problems, in the format described above. For each problem, they should provide a total of six points about that specific problem. To elaborate more on these points, the brief description instructs them to explain what the problem is so that we understand where to look for. The specific context where this problem can occur will give us a better idea of the cases we need to consider in order to approach solving this problem. The likely/actual causes will also indicate to us where our team made a poor choice in our initial design process, so that we are able to rethink our decisions. It is also asked of them to give us a severity estimation, which will be used by our team to prioritise which issues to prioritise. Finally, some suggestions for improvement are requested.

    The requirements are specified in such a specific and strict manner, so that not only we receive an evaluation of high quality, which can be used to obtain a different perspective compared to that of our team, but also so that the procedure can be repeated by other experts and/or teams, thus making our research replicable. In addition, the high level of detalization of the identified issues allows for more efficient development of mitigation method. As the evaluators also instructed to clearly indicate where the problem is by referencing slides and pictures, these will also be used to better understand the source of the problem. Our raw results will be in this format, and they will be used to improve our user interface in a way that makes it more convenient to use.
